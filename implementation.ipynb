{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "import sys\n",
    "from keras._tf_keras.keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, Bidirectional, LSTM, Dense, Dropout, Lambda, \n",
    "    Input, Concatenate, Reshape, Conv1D, Layer, Activation, \n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D, Multiply)\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import keras.src.legacy.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_evaluation(y_test, pred_test):\n",
    "\n",
    "    pred_matrix = np.zeros_like(pred_test, dtype=np.int8)\n",
    "\n",
    "    y_matrix = np.zeros_like(y_test, dtype=np.int8)\n",
    "    pred_indexs = np.argmax(pred_test, 1)\n",
    "    y_indexs = np.argmax(y_test, 1)\n",
    "\n",
    "    for i in range(len(pred_indexs)):\n",
    "        pred_matrix[i][pred_indexs[i]] = 1\n",
    "        y_matrix[i][y_indexs[i]] = 1\n",
    "\n",
    "    count_matrix = np.zeros((4, 3))\n",
    "    for class_idx in range(4):\n",
    "\n",
    "        count_matrix[class_idx][0] = np.sum(\n",
    "            np.array(pred_matrix[:, class_idx]) * np.array(y_matrix[:, class_idx]))  # tp\n",
    "        count_matrix[class_idx][1] = np.sum(np.array(\n",
    "            pred_matrix[:, class_idx]) * (1 - np.array(y_matrix[:, class_idx])))  # fp\n",
    "        count_matrix[class_idx][2] = np.sum(\n",
    "            (1 - np.array(pred_matrix[:, class_idx])) * np.array(y_matrix[:, class_idx]))  # fn\n",
    "\n",
    "    sumtp = sumfp = sumfn = 0\n",
    "\n",
    "    for i in range(4):\n",
    "        sumtp += count_matrix[i][0]\n",
    "        sumfp += count_matrix[i][1]\n",
    "        sumfn += count_matrix[i][2]\n",
    "\n",
    "        precision = recall = f1 = 0\n",
    "\n",
    "    if (sumtp + sumfp) == 0:\n",
    "        precision = 0.\n",
    "    else:\n",
    "        precision = float(sumtp) / (sumtp + sumfp)\n",
    "\n",
    "    if (sumtp + sumfn) == 0:\n",
    "        recall = 0.\n",
    "    else:\n",
    "        recall = float(sumtp) / (sumtp + sumfn)\n",
    "\n",
    "    if (precision + recall) == 0.:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test data which are pkl files\n",
    "\n",
    "with open('train.pkl', 'rb') as f_Train:\n",
    "    train_labels_vec = pkl.load(f_Train)\n",
    "    train_word = pkl.load(f_Train)\n",
    "\n",
    "    train_POS = pkl.load(f_Train)\n",
    "\n",
    "    train_distances = pkl.load(f_Train)\n",
    "\n",
    "    train_distances2 = pkl.load(f_Train)\n",
    "\n",
    "    train_shortest_word = pkl.load(f_Train)\n",
    "\n",
    "    train_shortest_pos = pkl.load(f_Train)\n",
    "    train_shortest_dis1 = pkl.load(f_Train)\n",
    "    train_shortest_dis2 = pkl.load(f_Train)\n",
    "\n",
    "    train_entity = pkl.load(f_Train)\n",
    "\n",
    "\n",
    "\n",
    "with open('test.pkl', 'rb') as f_Test:\n",
    "    test_labels_vec = pkl.load(f_Test)\n",
    "    test_word = pkl.load(f_Test)\n",
    "    test_POS = pkl.load(f_Test)\n",
    "    test_distances = pkl.load(f_Test)\n",
    "\n",
    "    test_distances2 = pkl.load(f_Test)\n",
    "\n",
    "    test_shortest_word = pkl.load(f_Test)\n",
    "\n",
    "    test_shortest_pos = pkl.load(f_Test)\n",
    "    test_shortest_dis1 = pkl.load(f_Test)\n",
    "    test_shortest_dis2 = pkl.load(f_Test)\n",
    "\n",
    "    test_entity = pkl.load(f_Test)\n",
    "\n",
    "\n",
    "with open('vec.pkl', 'rb') as f_word2vec:\n",
    "    vec_table = pkl.load(f_word2vec, encoding='latin1')\n",
    "    pos_vec_table = pkl.load(f_word2vec, encoding='latin1')\n",
    "    dis_vec_table = pkl.load(f_word2vec, encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_y = np.array(test_labels_vec, dtype=np.int8)\n",
    "train_y = np.array(train_labels_vec, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "(27792, 12)\n"
     ]
    }
   ],
   "source": [
    "s = {\n",
    "    'emb_dimension': 100,  # dimension of word embedding\n",
    "    'mini_batch': 64,\n",
    "    'shortest_part_length': 12,\n",
    "    'epochs': 60,\n",
    "    'class_type': 5,\n",
    "    'first_hidden_layer': 100,\n",
    "    'lr': 0.001,\n",
    "    'emb_dropout': 0.7,\n",
    "    'dense_dropout': 0.5,\n",
    "    'train_file': \"../data/train.pkl.gz\",\n",
    "    'test_file': \"../data/test.pkl.gz\",\n",
    "    'wordvecfile': \"../data/vec.pkl.gz\",\n",
    "\n",
    "}\n",
    "nb_class=s['class_type']\n",
    "test_word_bak = test_word\n",
    "\n",
    "word_dep_max = 0\n",
    "word_max_num = 0\n",
    "temp_max = 0\n",
    "\n",
    "new_train_word = []\n",
    "new_train_pos = []\n",
    "new_train_dis = []\n",
    "new_train_dis2 = []\n",
    "new_test_word = []\n",
    "new_test_pos = []\n",
    "new_test_dis = []\n",
    "new_test_dis2 = []\n",
    "\n",
    "word_length = []\n",
    "\n",
    "# calculate the max length of each subsequence\n",
    "for i in range(3):\n",
    "\n",
    "    temp_max = 0\n",
    "    temp_train_word = []\n",
    "    temp_train_pos = []\n",
    "    temp_train_dis = []\n",
    "    temp_train_dis2 = []\n",
    "    for j in range(len(train_word)):\n",
    "\n",
    "        assert len(train_word[j][i]) == len(train_POS[j][i])\n",
    "        assert len(train_POS[j][i]) == len(train_distances[j][i])\n",
    "        assert len(train_distances[j][i]) == len(train_distances2[j][i])\n",
    "        temp_train_word.append(train_word[j][i])\n",
    "        temp_train_pos.append(train_POS[j][i])\n",
    "        temp_train_dis.append(train_distances[j][i])\n",
    "        temp_train_dis2.append(train_distances2[j][i])\n",
    "        if len(train_word[j][i]) > temp_max:\n",
    "            temp_max = len(train_word[j][i])\n",
    "\n",
    "    new_train_word.append(temp_train_word)\n",
    "    new_train_pos.append(temp_train_pos)\n",
    "    new_train_dis.append(temp_train_dis)\n",
    "    new_train_dis2.append(temp_train_dis2)\n",
    "\n",
    "    temp_test_word = []\n",
    "    temp_test_pos = []\n",
    "    temp_test_dis = []\n",
    "    temp_test_dis2 = []\n",
    "    for j in range(len(test_word)):\n",
    "        assert len(test_word[j][i]) == len(test_POS[j][i])\n",
    "        assert len(test_POS[j][i]) == len(test_distances[j][i])\n",
    "        assert len(test_distances[j][i]) == len(test_distances2[j][i])\n",
    "        temp_test_word.append(test_word[j][i])\n",
    "        temp_test_pos.append(test_POS[j][i])\n",
    "        temp_test_dis.append(test_distances[j][i])\n",
    "        temp_test_dis2.append(test_distances2[j][i])\n",
    "        if len(test_word[j][i]) > temp_max:\n",
    "            temp_max = len(test_word[j][i])\n",
    "\n",
    "    new_test_word.append(temp_test_word)\n",
    "    new_test_pos.append(temp_test_pos)\n",
    "    new_test_dis.append(temp_test_dis)\n",
    "    new_test_dis2.append(temp_test_dis2)\n",
    "    word_length.append(temp_max)\n",
    "\n",
    "train_word = new_train_word\n",
    "\n",
    "train_POS = new_train_pos\n",
    "train_distances = new_train_dis\n",
    "train_distances2 = new_train_dis2\n",
    "\n",
    "test_word = new_test_word\n",
    "\n",
    "test_POS = new_test_pos\n",
    "test_distances = new_test_dis\n",
    "test_distances2 = new_test_dis2\n",
    "\n",
    "train_entity_0 = []\n",
    "train_entity_1 = []\n",
    "test_entity_0 = []\n",
    "test_entity_1 = []\n",
    "for i in range(len(train_entity)):\n",
    "    temp_list = []\n",
    "    temp_list.append(train_entity[i][0])\n",
    "    train_entity_0.append(temp_list)\n",
    "    temp_list = []\n",
    "    temp_list.append(train_entity[i][1])\n",
    "    train_entity_1.append(temp_list)\n",
    "for i in range(len(test_entity)):\n",
    "    temp_list = []\n",
    "    temp_list.append(test_entity[i][0])\n",
    "    test_entity_0.append(temp_list)\n",
    "    temp_list = []\n",
    "    temp_list.append(test_entity[i][1])\n",
    "    test_entity_1.append(temp_list)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "\n",
    "train_word_list = []\n",
    "test_word_list = []\n",
    "train_POS_list = []\n",
    "test_POS_list = []\n",
    "\n",
    "train_distances_list = []\n",
    "test_distances_list = []\n",
    "train_distances2_list = []\n",
    "test_distances2_list = []\n",
    "\n",
    "for i in range(len(train_word)):\n",
    "    train_word_list.append(\n",
    "        sequence.pad_sequences(train_word[i], maxlen=word_length[i], truncating='post', padding='post'))\n",
    "for i in range(len(test_word)):\n",
    "    test_word_list.append(sequence.pad_sequences(test_word[i], maxlen=word_length[i], truncating='post',\n",
    "                                                 padding='post'))\n",
    "\n",
    "for i in range(len(train_POS)):\n",
    "    train_POS_list.append(\n",
    "        sequence.pad_sequences(train_POS[i], maxlen=word_length[i], truncating='post', padding='post'))\n",
    "for i in range(len(test_distances)):\n",
    "    test_POS_list.append(sequence.pad_sequences(test_POS[i], maxlen=word_length[i], truncating='post',\n",
    "                                                padding='post'))\n",
    "\n",
    "for i in range(len(train_distances)):\n",
    "    train_distances_list.append(\n",
    "        sequence.pad_sequences(train_distances[i], maxlen=word_length[i], truncating='post', padding='post'))\n",
    "for i in range(len(test_distances)):\n",
    "    test_distances_list.append(sequence.pad_sequences(test_distances[i], maxlen=word_length[i], truncating='post',\n",
    "                                                      padding='post'))\n",
    "\n",
    "for i in range(len(train_distances2)):\n",
    "    train_distances2_list.append(\n",
    "        sequence.pad_sequences(train_distances2[i], maxlen=word_length[i], truncating='post', padding='post'))\n",
    "for i in range(len(test_distances2)):\n",
    "    test_distances2_list.append(sequence.pad_sequences(test_distances2[i], maxlen=word_length[i], truncating='post',\n",
    "                                                       padding='post'))\n",
    "\n",
    "train_shortest_word = sequence.pad_sequences(\n",
    "    train_shortest_word, maxlen=s['shortest_part_length'], truncating='post', padding='post')\n",
    "test_shortest_word = sequence.pad_sequences(test_shortest_word, maxlen=s['shortest_part_length'], truncating='post',\n",
    "                                            padding='post')\n",
    "\n",
    "train_shortest_pos = sequence.pad_sequences(\n",
    "    train_shortest_pos, maxlen=s['shortest_part_length'], truncating='post', padding='post')\n",
    "test_shortest_pos = sequence.pad_sequences(\n",
    "    test_shortest_pos, maxlen=s['shortest_part_length'], truncating='post', padding='post')\n",
    "train_shortest_dis1 = sequence.pad_sequences(\n",
    "    train_shortest_dis1, maxlen=s['shortest_part_length'], truncating='post', padding='post')\n",
    "test_shortest_dis1 = sequence.pad_sequences(\n",
    "    test_shortest_dis1, maxlen=s['shortest_part_length'], truncating='post', padding='post')\n",
    "train_shortest_dis2 = sequence.pad_sequences(\n",
    "    train_shortest_dis2, maxlen=s['shortest_part_length'], truncating='post', padding='post')\n",
    "test_shortest_dis2 = sequence.pad_sequences(\n",
    "    test_shortest_dis2, maxlen=s['shortest_part_length'], truncating='post', padding='post')\n",
    "\n",
    "train_entity_0 = sequence.pad_sequences(\n",
    "    train_entity_0, maxlen=1, truncating='post', padding='post')\n",
    "train_entity_1 = sequence.pad_sequences(\n",
    "    train_entity_1, maxlen=1, truncating='post', padding='post')\n",
    "test_entity_0 = sequence.pad_sequences(\n",
    "    test_entity_0, maxlen=1, truncating='post', padding='post')\n",
    "test_entity_1 = sequence.pad_sequences(\n",
    "    test_entity_1, maxlen=1, truncating='post', padding='post')\n",
    "\n",
    "print(train_shortest_word.shape)\n",
    "\n",
    "\n",
    "# embedding layer\n",
    "\n",
    "wordembedding = Embedding(vec_table.shape[0],\n",
    "                          vec_table.shape[1],\n",
    "                          weights=[vec_table])\n",
    "\n",
    "disembedding = Embedding(dis_vec_table.shape[0],\n",
    "                         dis_vec_table.shape[1],\n",
    "                         weights=[dis_vec_table]\n",
    "                         )\n",
    "\n",
    "posembedding = Embedding(pos_vec_table.shape[0],\n",
    "                         pos_vec_table.shape[1],\n",
    "                         weights=[pos_vec_table]\n",
    "                         )\n",
    "\n",
    "input_entity_0 = Input(shape=(1,), dtype='int32', name='input_entity_0')\n",
    "entity_fea_0 = wordembedding(input_entity_0)\n",
    "input_entity_1 = Input(shape=(1,), dtype='int32', name='input_entity_1')\n",
    "entity_fea_1 = wordembedding(input_entity_1)\n",
    "\n",
    "input_word_0 = Input(\n",
    "    shape=(word_length[0],), dtype='int32', name='input_word_0')\n",
    "word_fea_0 = wordembedding(input_word_0)  # trainable=False\n",
    "\n",
    "input_pos_0 = Input(\n",
    "    shape=(word_length[0],), dtype='int32', name='input_pos_0')\n",
    "pos_fea_0 = posembedding(input_pos_0)\n",
    "\n",
    "input_dis1_0 = Input(\n",
    "    shape=(word_length[0],), dtype='int32', name='input_dis1_0')\n",
    "dis_fea1_0 = disembedding(input_dis1_0)\n",
    "\n",
    "input_dis2_0 = Input(\n",
    "    shape=(word_length[0],), dtype='int32', name='input_dis2_0')\n",
    "dis_fea2_0 = disembedding(input_dis2_0)\n",
    "\n",
    "input_word_1 = Input(\n",
    "    shape=(word_length[1],), dtype='int32', name='input_word_1')\n",
    "word_fea_1 = wordembedding(input_word_1)\n",
    "\n",
    "input_pos_1 = Input(\n",
    "    shape=(word_length[1],), dtype='int32', name='input_pos_1')\n",
    "pos_fea_1 = posembedding(input_pos_1)\n",
    "\n",
    "input_dis1_1 = Input(\n",
    "    shape=(word_length[1],), dtype='int32', name='input_dis1_1')\n",
    "dis_fea1_1 = disembedding(input_dis1_1)\n",
    "\n",
    "input_dis2_1 = Input(\n",
    "    shape=(word_length[1],), dtype='int32', name='input_dis2_1')\n",
    "dis_fea2_1 = disembedding(input_dis2_1)\n",
    "\n",
    "input_word_2 = Input(\n",
    "    shape=(word_length[2],), dtype='int32', name='input_word_2')\n",
    "word_fea_2 = wordembedding(input_word_2)\n",
    "\n",
    "input_pos_2 = Input(\n",
    "    shape=(word_length[2],), dtype='int32', name='input_pos_2')\n",
    "pos_fea_2 = posembedding(input_pos_2)\n",
    "\n",
    "input_dis1_2 = Input(\n",
    "    shape=(word_length[2],), dtype='int32', name='input_dis1_2')\n",
    "dis_fea1_2 = disembedding(input_dis1_2)\n",
    "\n",
    "input_dis2_2 = Input(\n",
    "    shape=(word_length[2],), dtype='int32', name='input_dis2_2')\n",
    "dis_fea2_2 = disembedding(input_dis2_2)\n",
    "\n",
    "emb_merge_0 = Concatenate()(\n",
    "    [word_fea_0, pos_fea_0, dis_fea1_0, dis_fea2_0])\n",
    "\n",
    "emb_merge_1 = Concatenate()(\n",
    "    [word_fea_1, pos_fea_1, dis_fea1_1, dis_fea2_1])\n",
    "emb_merge_2 = Concatenate()(\n",
    "    [word_fea_2, pos_fea_2, dis_fea1_2, dis_fea2_2])\n",
    "\n",
    "input_shortest_word = Input(\n",
    "    shape=(s['shortest_part_length'],), dtype='int32', name='input_shortest_word')\n",
    "shortest_word_fea = wordembedding(input_shortest_word)\n",
    "\n",
    "input_shortest_pos = Input(\n",
    "    shape=(s['shortest_part_length'],), dtype='int32', name='input_shortest_pos')\n",
    "\n",
    "shortest_pos_fea = posembedding(input_shortest_pos)\n",
    "\n",
    "shortest_input_dis1 = Input(\n",
    "    shape=(s['shortest_part_length'],), dtype='int32', name='shortest_input_dis1')\n",
    "shortest_dis_fea1 = disembedding(shortest_input_dis1)\n",
    "\n",
    "shortest_input_dis2 = Input(\n",
    "    shape=(s['shortest_part_length'],), dtype='int32', name='shortest_input_dis2')\n",
    "shortest_dis_fea2 = disembedding(shortest_input_dis2)\n",
    "\n",
    "emb_merge_shortest = Concatenate()(\n",
    "    [shortest_word_fea, shortest_pos_fea, shortest_dis_fea1, shortest_dis_fea2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 60, 200), (None, 1, 200), (None, 1, 200), (None, 60, 230)]\n",
      "60\n",
      "[(None, 60, 200), (None, 1, 200), (None, 1, 200), (None, 60, 230)]\n",
      "60\n",
      "[(None, 60, 200), (None, 1, 200), (None, 1, 200), (None, 60, 230)]\n",
      "60\n",
      "[(None, 12, 200), (None, 1, 200), (None, 1, 200), (None, 12, 230)]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "class emb_AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        super(emb_AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        self.scaler=input_shape[0][1]\n",
    "        print(self.scaler)\n",
    "\n",
    "        super(emb_AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        sentence = x[0]\n",
    "        entity_0=x[1]\n",
    "        entity_1=x[2]\n",
    "\n",
    "        eij_0=K.batch_dot(sentence,entity_0,axes=[2,2])\n",
    "        K.sqrt(sentence)\n",
    "        K.sqrt(entity_0)\n",
    "        eij_0/=2000\n",
    "        # print('eij', eij_0.shape, x[0].shape[0],x[0].shape[1], tf.reshape(eij_0, (-1, self.scaler)))\n",
    "        eij_0 = tf.reshape(eij_0, (-1, self.scaler))\n",
    "        ai_0 = K.exp(eij_0)\n",
    "        weights_0=ai_0 / tf.expand_dims(K.sum(ai_0, axis=1), axis=-1)\n",
    "\n",
    "        eij_1 = K.batch_dot(sentence, entity_1, axes=[2, 2])\n",
    "        eij_1 /= 2000\n",
    "        eij_1 = tf.reshape(eij_1, (-1, self.scaler))\n",
    "        ai_1 = K.exp(eij_1)\n",
    "        weights_1 = ai_1 / tf.expand_dims(K.sum(ai_1, axis=1), axis=-1)\n",
    "\n",
    "\n",
    "        weights=((weights_0+weights_1)/2.0)*self.scaler\n",
    "\n",
    "        weighted_input = x[3] * tf.expand_dims(weights, axis=-1)\n",
    "\n",
    "        return weighted_input\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[3])\n",
    "\n",
    "\n",
    "# attention layer\n",
    "att_emb_merge_0 = emb_AttentionLayer()(\n",
    "    [word_fea_0, entity_fea_0, entity_fea_1, emb_merge_0])\n",
    "att_emb_merge_1 = emb_AttentionLayer()(\n",
    "    [word_fea_1, entity_fea_0, entity_fea_1, emb_merge_1])\n",
    "att_emb_merge_2 = emb_AttentionLayer()(\n",
    "    [word_fea_2, entity_fea_0, entity_fea_1, emb_merge_2])\n",
    "att_emb_merge_shortest = emb_AttentionLayer()(\n",
    "    [shortest_word_fea, entity_fea_0, entity_fea_1, emb_merge_shortest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CBAM_EntityAttentionLayer(Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(CBAM_EntityAttentionLayer, self).__init__(**kwargs)\n",
    "#         self.shared_dense_one = None\n",
    "#         self.shared_dense_two = None\n",
    "#         self.spatial_conv = None\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.scaler = input_shape[0][1]\n",
    "#         channel_shape = input_shape[3][-1]\n",
    "\n",
    "#         self.shared_dense_one = Dense(units=channel_shape // 8, activation='relu')\n",
    "#         self.shared_dense_two = Dense(units=channel_shape, activation='sigmoid')\n",
    "#         self.spatial_conv = Conv1D(filters=1, kernel_size=1, activation='sigmoid')\n",
    "\n",
    "#         super(CBAM_EntityAttentionLayer, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs, mask=None):\n",
    "#         sentence, entity_, entity_1, merged_features = inputs\n",
    "\n",
    "#         # Calculate entity distance-based attention weights\n",
    "#         eij_ = K.batch_dot(sentence, entity_, axes=[2, 2]) / 200\n",
    "#         eij_ = tf.reshape(eij_, (-1, self.scaler))\n",
    "#         ai_ = K.exp(eij_)\n",
    "#         weights_ = ai_ / tf.expand_dims(K.sum(ai_, axis=1), axis=-1)\n",
    "\n",
    "#         eij_1 = K.batch_dot(sentence, entity_1, axes=[2, 2]) / 200\n",
    "#         eij_1 = tf.reshape(eij_1, (-1, self.scaler))\n",
    "#         ai_1 = K.exp(eij_1)\n",
    "#         weights_1 = ai_1 / tf.expand_dims(K.sum(ai_1, axis=1), axis=-1)\n",
    "\n",
    "#         weights = ((weights_ + weights_1) / 2.) * self.scaler\n",
    "\n",
    "#         # Apply channel attention\n",
    "#         avg_pool = GlobalAveragePooling1D()(merged_features)\n",
    "#         max_pool = GlobalMaxPooling1D()(merged_features)\n",
    "\n",
    "#         avg_attention = self.shared_dense_two(self.shared_dense_one(avg_pool))\n",
    "#         max_attention = self.shared_dense_two(self.shared_dense_one(max_pool))\n",
    "        \n",
    "#         channel_attention = Multiply()([merged_features, tf.expand_dims(avg_attention + max_attention, axis=1)])\n",
    "\n",
    "#         # Apply spatial attention\n",
    "#         avg_pool_spatial = tf.reduce_mean(channel_attention, axis=-1, keepdims=True)\n",
    "#         max_pool_spatial = tf.reduce_max(channel_attention, axis=-1, keepdims=True)\n",
    "#         concat = tf.concat([avg_pool_spatial, max_pool_spatial], axis=-1)\n",
    "        \n",
    "#         spatial_attention = self.spatial_conv(concat)\n",
    "#         spatial_attention = Multiply()([spatial_attention, tf.expand_dims(weights, -1)])\n",
    "\n",
    "#         weighted_input = Multiply()([channel_attention, spatial_attention])\n",
    "\n",
    "#         return weighted_input\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return input_shape[3]\n",
    "\n",
    "\n",
    "# # attention layer\n",
    "# att_emb_merge_0 = CBAM_EntityAttentionLayer()(\n",
    "#     [word_fea_0, entity_fea_0, entity_fea_1, emb_merge_0])\n",
    "# att_emb_merge_1 = CBAM_EntityAttentionLayer()(\n",
    "#     [word_fea_1, entity_fea_0, entity_fea_1, emb_merge_1])\n",
    "# att_emb_merge_2 = CBAM_EntityAttentionLayer()(\n",
    "#     [word_fea_2, entity_fea_0, entity_fea_1, emb_merge_2])\n",
    "# att_emb_merge_shortest = CBAM_EntityAttentionLayer()(\n",
    "#     [shortest_word_fea, entity_fea_0, entity_fea_1, emb_merge_shortest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cbam_attention_layer(inputs):\n",
    "#     word_features, entity_features_, entity_features_1, merged_features = inputs\n",
    "    \n",
    "#     # Example Channel Attention applied on word features\n",
    "#     channel_avg_pool = GlobalAveragePooling1D()(merged_features)\n",
    "#     channel_max_pool = GlobalMaxPooling1D()(merged_features)\n",
    "    \n",
    "#     shared_dense_one = Dense(units=channel_avg_pool.shape[-1] // 8, activation='relu')\n",
    "#     shared_dense_two = Dense(units=channel_avg_pool.shape[-1], activation='sigmoid')\n",
    "\n",
    "#     avg_attention_scores = shared_dense_two(shared_dense_one(channel_avg_pool))\n",
    "#     max_attention_scores = shared_dense_two(shared_dense_one(channel_max_pool))\n",
    "\n",
    "#     channel_attention_scores = Add()([avg_attention_scores, max_attention_scores])\n",
    "    \n",
    "#     # Channel Attention\n",
    "#     channel_attention_scores = Lambda(lambda x: x[:, None, :])(channel_attention_scores)  # Add a dimension\n",
    "#     channel_attention_output = Multiply()([merged_features, channel_attention_scores])\n",
    "    \n",
    "#     # Example Spatial Attention\n",
    "#     avg_pool = Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(channel_attention_output)\n",
    "#     max_pool = Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(channel_attention_output)\n",
    "#     concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "    \n",
    "#     spatial_attention_scores = tf.keras.layers.Conv1D(filters=1, kernel_size=1, activation='sigmoid')(concat)\n",
    "#     spatial_attention_output = Multiply()([channel_attention_output, spatial_attention_scores])\n",
    "    \n",
    "#     return spatial_attention_output\n",
    "\n",
    "# # attention layer\n",
    "# att_emb_merge_0 = cbam_attention_layer(\n",
    "#     [word_fea_0, entity_fea_0, entity_fea_1, emb_merge_0])\n",
    "# att_emb_merge_1 = cbam_attention_layer(\n",
    "#     [word_fea_1, entity_fea_0, entity_fea_1, emb_merge_1])\n",
    "# att_emb_merge_2 = cbam_attention_layer(\n",
    "#     [word_fea_2, entity_fea_0, entity_fea_1, emb_merge_2])\n",
    "# att_emb_merge_shortest = cbam_attention_layer(\n",
    "#     [shortest_word_fea, entity_fea_0, entity_fea_1, emb_merge_shortest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout layer\n",
    "emb_merge_0 = Dropout(s['emb_dropout'])(att_emb_merge_0)\n",
    "emb_merge_1 = Dropout(s['emb_dropout'])(att_emb_merge_1)\n",
    "emb_merge_2 = Dropout(s['emb_dropout'])(att_emb_merge_2)\n",
    "emb_merge_shortest = Dropout(s['emb_dropout'])(att_emb_merge_shortest)\n",
    "# LSTM Layers\n",
    "gru_merge_0 = Bidirectional(LSTM(units=100))(emb_merge_0)\n",
    "gru_merge_1 = Bidirectional(LSTM(units=100))(emb_merge_1)\n",
    "gru_merge_2 = Bidirectional(LSTM(units=100))(emb_merge_2)\n",
    "gru_merge_shortest = Bidirectional(LSTM(units=100))(emb_merge_shortest)\n",
    "\n",
    "# lstm_out = Bidirectional(LSTM(units=100))(emb_merge_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "gru_merge_0 = Reshape((1, 2*s['first_hidden_layer']))(gru_merge_0)\n",
    "gru_merge_1 = Reshape((1, 2*s['first_hidden_layer']))(gru_merge_1)\n",
    "gru_merge_2 = Reshape((1, 2*s['first_hidden_layer']))(gru_merge_2)\n",
    "gru_merge_shortest = Reshape(\n",
    "    (1, 2*s['first_hidden_layer']))(gru_merge_shortest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_merge_temp = merged = Concatenate()([gru_merge_0, entity_fea_0,gru_merge_1,gru_merge_shortest,entity_fea_1,gru_merge_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_merge_top = Bidirectional(LSTM(units=100))(gru_merge_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_drop_0=Dropout(s['dense_dropout'])(gru_merge_top)\n",
    "\n",
    "final_output=Dense(nb_class, activation='softmax')(dense_drop_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_word_0,input_word_1,input_word_2,input_pos_0,input_pos_1,input_pos_2,input_dis1_0,input_dis1_1,input_dis1_2,\n",
    "                         input_dis2_0,input_dis2_1,input_dis2_2,input_shortest_word, shortest_input_dis1,shortest_input_dis2,\n",
    "                         input_shortest_pos,input_entity_0,input_entity_1], final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(optimizer=RMSprop(learning_rate=.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amir/Programming/University/NLP_Class/venv/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_word_0', 'input_word_1', 'input_word_2', 'input_pos_0', 'input_pos_1', 'input_pos_2', 'input_dis1_0', 'input_dis1_1', 'input_dis1_2', 'input_dis2_0', 'input_dis2_1', 'input_dis2_2', 'input_shortest_word', 'shortest_input_dis1', 'shortest_input_dis2', 'input_shortest_pos', 'input_entity_0', 'input_entity_1']. Received: the structure of inputs=('*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 75ms/step - accuracy: 0.8426 - loss: 0.5770\n",
      "Epoch 2/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 77ms/step - accuracy: 0.8548 - loss: 0.5319\n",
      "Epoch 3/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 76ms/step - accuracy: 0.8533 - loss: 0.5020\n",
      "Epoch 4/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 77ms/step - accuracy: 0.8544 - loss: 0.4812\n",
      "Epoch 5/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 77ms/step - accuracy: 0.8560 - loss: 0.4788\n",
      "Epoch 6/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8582 - loss: 0.4717\n",
      "Epoch 7/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 76ms/step - accuracy: 0.8577 - loss: 0.4627\n",
      "Epoch 8/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 75ms/step - accuracy: 0.8601 - loss: 0.4504\n",
      "Epoch 9/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 77ms/step - accuracy: 0.8609 - loss: 0.4559\n",
      "Epoch 10/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.8583 - loss: 0.4528\n",
      "Epoch 11/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 79ms/step - accuracy: 0.8639 - loss: 0.4390\n",
      "Epoch 12/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8567 - loss: 0.4436\n",
      "Epoch 13/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.8599 - loss: 0.4380\n",
      "Epoch 14/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.8570 - loss: 0.4407\n",
      "Epoch 15/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8602 - loss: 0.4435\n",
      "Epoch 16/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.8621 - loss: 0.4290\n",
      "Epoch 17/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.8618 - loss: 0.4242\n",
      "Epoch 18/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8654 - loss: 0.4164\n",
      "Epoch 19/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 81ms/step - accuracy: 0.8660 - loss: 0.4141\n",
      "Epoch 20/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8696 - loss: 0.3957\n",
      "Epoch 21/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8640 - loss: 0.4136\n",
      "Epoch 22/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8669 - loss: 0.4040\n",
      "Epoch 23/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8660 - loss: 0.3975\n",
      "Epoch 24/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8732 - loss: 0.3702\n",
      "Epoch 25/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.8730 - loss: 0.3782\n",
      "Epoch 26/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8754 - loss: 0.3674\n",
      "Epoch 27/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8745 - loss: 0.3719\n",
      "Epoch 28/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8769 - loss: 0.3594\n",
      "Epoch 29/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.8795 - loss: 0.3532\n",
      "Epoch 30/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.8801 - loss: 0.3447\n",
      "Epoch 31/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.8793 - loss: 0.3464\n",
      "Epoch 32/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8799 - loss: 0.3485\n",
      "Epoch 33/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.8809 - loss: 0.3396\n",
      "Epoch 34/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 77ms/step - accuracy: 0.8860 - loss: 0.3374\n",
      "Epoch 35/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 77ms/step - accuracy: 0.8828 - loss: 0.3420\n",
      "Epoch 36/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8879 - loss: 0.3252\n",
      "Epoch 37/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 77ms/step - accuracy: 0.8890 - loss: 0.3155\n",
      "Epoch 38/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 77ms/step - accuracy: 0.8911 - loss: 0.3124\n",
      "Epoch 39/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.8904 - loss: 0.3151\n",
      "Epoch 40/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.8931 - loss: 0.3069\n",
      "Epoch 41/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.8952 - loss: 0.3085\n",
      "Epoch 42/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.8986 - loss: 0.2965\n",
      "Epoch 43/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.9023 - loss: 0.2868\n",
      "Epoch 44/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.9025 - loss: 0.2938\n",
      "Epoch 45/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.9024 - loss: 0.2848\n",
      "Epoch 46/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.9029 - loss: 0.2770\n",
      "Epoch 47/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.9093 - loss: 0.2726\n",
      "Epoch 48/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.9093 - loss: 0.2699\n",
      "Epoch 49/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.9102 - loss: 0.2673\n",
      "Epoch 50/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.9133 - loss: 0.2600\n",
      "Epoch 51/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.9093 - loss: 0.2675\n",
      "Epoch 52/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.9101 - loss: 0.2656\n",
      "Epoch 53/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 77ms/step - accuracy: 0.9187 - loss: 0.2480\n",
      "Epoch 54/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 77ms/step - accuracy: 0.9159 - loss: 0.2525\n",
      "Epoch 55/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.9156 - loss: 0.2505\n",
      "Epoch 56/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.9172 - loss: 0.2524\n",
      "Epoch 57/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 78ms/step - accuracy: 0.9240 - loss: 0.2354\n",
      "Epoch 58/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.9147 - loss: 0.2545\n",
      "Epoch 59/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 77ms/step - accuracy: 0.9212 - loss: 0.2452\n",
      "Epoch 60/60\n",
      "\u001b[1m435/435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 79ms/step - accuracy: 0.9215 - loss: 0.2352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x36ee08040>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[train_word_list[0], train_word_list[1], train_word_list[2], train_POS_list[0], train_POS_list[1], train_POS_list[2],\n",
    "                          train_distances_list[0], train_distances_list[1], train_distances_list[2], train_distances2_list[0],\n",
    "                          train_distances2_list[1], train_distances2_list[2], train_shortest_word, train_shortest_dis1, \n",
    "                           train_shortest_dis2, train_shortest_pos, train_entity_0, train_entity_1], y=train_y, batch_size=64, shuffle=True, epochs=60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
      "testing epochs:60 precision:0.65134 recall:0.79571 F1:0.71632\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict([test_word_list[0], test_word_list[1], test_word_list[2], test_POS_list[0], test_POS_list[1], test_POS_list[2],\n",
    "                          test_distances_list[0], test_distances_list[1], test_distances_list[2], test_distances2_list[0],\n",
    "                          test_distances2_list[1], test_distances2_list[2], test_shortest_word, test_shortest_dis1, test_shortest_dis2, test_shortest_pos, test_entity_0, test_entity_1],\n",
    "                          batch_size=s['mini_batch'])  # ,test_POS\n",
    "\n",
    "precision, recall, F1 = result_evaluation(answer_y, pred_test)\n",
    "# print(testing epochs:' + ' precision:' + str(np.round(precision, 5)) + ' recall:' + str(np.round(recall, 5)) + ' F1:' + str(np.round(F1, 5))\n",
    "\n",
    "print('testing epochs:' + str(60)+' precision:' + str(np.round(precision, 5)\n",
    "                                                         ) + ' recall:' + str(np.round(recall, 5)) + ' F1:' + str(np.round(F1, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amir/Programming/University/NLP_Class/venv/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_word_0', 'input_word_1', 'input_word_2', 'input_pos_0', 'input_pos_1', 'input_pos_2', 'input_dis1_0', 'input_dis1_1', 'input_dis1_2', 'input_dis2_0', 'input_dis2_1', 'input_dis2_2', 'input_shortest_word', 'shortest_input_dis1', 'shortest_input_dis2', 'input_shortest_pos', 'input_entity_0', 'input_entity_1']. Received: the structure of inputs=('*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict([test_word_list[0], test_word_list[1], test_word_list[2], test_POS_list[0], test_POS_list[1], test_POS_list[2],\n",
    "                          test_distances_list[0], test_distances_list[1], test_distances_list[2], test_distances2_list[0],\n",
    "                          test_distances2_list[1], test_distances2_list[2], test_shortest_word, test_shortest_dis1, test_shortest_dis2, test_shortest_pos, test_entity_0, test_entity_1],\n",
    "                          batch_size=s['mini_batch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # np.argmax(pred_test.round(2), 1)\n",
    "# pred_test.round(2)\n",
    "# y_test = np.zeros_like(pred_test)\n",
    "# max_indices = np.argmax(pred_test, axis=1)\n",
    "# y_test[np.arange(pred_test.shape[0]), max_indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.86      0.71       360\n",
      "           1       0.62      0.80      0.70       302\n",
      "           2       0.79      0.85      0.82       221\n",
      "           3       0.71      0.43      0.53        96\n",
      "           4       0.97      0.92      0.95      4390\n",
      "\n",
      "    accuracy                           0.90      5369\n",
      "   macro avg       0.74      0.77      0.74      5369\n",
      "weighted avg       0.91      0.90      0.90      5369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_y = np.argmax(answer_y, axis=1)\n",
    "pred_test = np.argmax(pred_test, axis=1)\n",
    "\n",
    "# precision = precision_score(answer_y, y_test)#, average='weighted')\n",
    "# recall = recall_score(answer_y, y_test)#, average='weighted')\n",
    "# f1 = f1_score(answer_y, y_test)#, average='weighted')\n",
    "\n",
    "# Print the results\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "# print(f'Recall: {recall:.2f}')\n",
    "# print(f'F1-Score: {f1:.2f}')\n",
    "print(classification_report(answer_y, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
